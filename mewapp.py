# import streamlit as st
# import joblib
# import numpy as np
# from sklearn.metrics.pairwise import cosine_similarity

# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∏–¥–µ–Ω—Ç–∏—á–Ω–∞ —Ç–æ–π, —á—Ç–æ –≤ –æ–±—É—á–µ–Ω–∏–∏)
# def clean_text(text):
#     import re
#     from simplemma import lemmatize
#     from nltk.corpus import stopwords
#     stop_words = set(stopwords.words('russian') + stopwords.words('english'))
#     text = re.sub(r"[^–∞-—è–ê-–Øa-zA-Z0-9\s]", "", text)
#     text = text.lower()
#     words = text.split()
#     cleaned_words = [lemmatize(word, 'ru') for word in words if word not in stop_words]
#     return " ".join(cleaned_words)

# # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤
# vectorizer = joblib.load('tfidf_vectorizer.pkl')
# label_encoder = joblib.load('label_encoder.pkl')
# all_titles_tfidf = joblib.load('all_titles_tfidf.pkl')
# articles_data = joblib.load('articles_data.pkl')

# # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
# models = {
#     "DecisionTreeClassifier": joblib.load('DecisionTreeClassifier.pkl'),
#     "KNeighborsClassifier": joblib.load('KNeighborsClassifier.pkl'),
#     "MultinomialNB": joblib.load('MultinomialNB.pkl'),
#     "CatBoostClassifier": joblib.load('CatBoostClassifier.pkl'),
#     "LogisticRegression": joblib.load('LogisticRegression.pkl'),
#     "SVC": joblib.load('SVC.pkl')
# }

# # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
# st.title("üîç –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–∑–¥–µ–ª –Ω–∞—É–∫–∏ –∏ –Ω–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ —Å—Ç–∞—Ç—å–∏")

# # –í–≤–æ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç–∞—Ç—å–∏
# user_input = st.text_input("–í–≤–µ–¥–∏—Ç–µ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤–∞—à–µ–π —Å—Ç–∞—Ç—å–∏:")

# if st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å"):
#     if user_input:
#         # –û—á–∏—Å—Ç–∫–∞ –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤–≤–µ–¥–µ–Ω–Ω–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
#         cleaned_input = clean_text(user_input)
#         input_tfidf = vectorizer.transform([cleaned_input]).toarray()

#         # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
#         results = {}
#         for model_name, model in models.items():
#             predicted_class = model.predict(input_tfidf)[0]
#             results[model_name] = label_encoder.inverse_transform([predicted_class])[0]

#         # –í—ã–≤–æ–¥ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
#         st.write("### –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã –Ω–∞—É–∫–∏:")
#         for model, category in results.items():
#             st.write(f"{model}: {category}")

#         # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π
#         similarities = cosine_similarity(input_tfidf, all_titles_tfidf)[0]
#         top_indices = np.argsort(similarities)[::-1][:5]  # –¢–æ–ø-5 –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞

#         st.write("### –¢–æ–ø-5 –ø–æ—Ö–æ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π:")
#         for idx in top_indices:
#             similarity_score = similarities[idx]
#             if similarity_score > 0:  # –í—ã–≤–æ–¥–∏–º —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç—å–∏ —Å –Ω–µ–Ω—É–ª–µ–≤—ã–º —Å—Ö–æ–¥—Å—Ç–≤–æ–º
#                 title = articles_data.iloc[idx]['title']
#                 category = label_encoder.inverse_transform([articles_data.iloc[idx]['category']])[0]
#                 st.write(f"- **{title}** (–†–∞–∑–¥–µ–ª: {category}, –°—Ö–æ–¥—Å—Ç–≤–æ: {similarity_score:.4f})")
#             else:
#                 st.write("–ù–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π –≤ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö.")
#                 break

#     else:
#         st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏.")

# st.text("*–≥–¥–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞ - 0, –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ - 1, —Ñ–∏–∑–∏–∫–∞ - 2, —Ö–∏–º–∏—è - 3")
# st.markdown("""
#     ---
#     –°–¥–µ–ª–∞–Ω–æ —Å ‚ù§Ô∏è –∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Streamlit –∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
# """)


import streamlit as st
import joblib
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import pandas as pd
import re
from simplemma import lemmatize
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')

# import locale
# locale.setlocale(locale.LC_ALL, 'ru_RU.UTF-8') 

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
def clean_text(text):
    stop_words = set(stopwords.words('russian') + stopwords.words('english'))
    text = re.sub(r"[^–∞-—è–ê-–Øa-zA-Z0-9\s]", "", text)
    text = text.lower()
    words = text.split()
    cleaned_words = [lemmatize(word, 'ru') for word in words if word not in stop_words]
    return " ".join(cleaned_words)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
def get_top_keywords(input_tfidf, vectorizer, top_n=5):
    feature_names = vectorizer.get_feature_names_out()
    tfidf_scores = input_tfidf.toarray()[0]
    top_indices = np.argsort(tfidf_scores)[::-1][:top_n]
    return [feature_names[idx] for idx in top_indices if tfidf_scores[idx] > 0]

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–±–ª–∞–∫–∞ —Å–ª–æ–≤
def create_word_cloud(text):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    st.pyplot(plt)

# –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤
vectorizer = joblib.load('tfidf_vectorizer.pkl')
label_encoder = joblib.load('label_encoder.pkl')
all_titles_tfidf = joblib.load('all_titles_tfidf.pkl')
articles_data = joblib.load('articles_data.pkl')
articles = pd.read_csv('df_russian_articles.csv')
articles.drop(columns=["Unnamed: 0", "Unnamed: 0.1", "lang"], inplace=True)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
models = {
    "DecisionTreeClassifier": joblib.load('DecisionTreeClassifier.pkl'),
    "KNeighborsClassifier": joblib.load('KNeighborsClassifier.pkl'),
    "MultinomialNB": joblib.load('MultinomialNB.pkl'),
    "CatBoostClassifier": joblib.load('CatBoostClassifier.pkl'),
    "LogisticRegression": joblib.load('LogisticRegression.pkl'),
    "SVC": joblib.load('SVC.pkl')
}

# –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
st.set_page_config(page_title="–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–µ–π", page_icon="üîç")
st.title("üîç –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞—É—á–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ –∏ –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π")
st.subheader("–í–≤–µ–¥–∏—Ç–µ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤–∞—à–µ–π —Å—Ç–∞—Ç—å–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

# –í—ã–≤–æ–¥ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å—Ç–∞—Ç–µ–π
st.write("### –î–∞—Ç–∞—Å–µ—Ç —Å—Ç–∞—Ç–µ–π:")
st.dataframe(articles)

# –í–≤–æ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç–∞—Ç—å–∏
user_input = st.text_input("–í–≤–µ–¥–∏—Ç–µ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤–∞—à–µ–π —Å—Ç–∞—Ç—å–∏:")

if st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å"):
    if user_input:
        # –û—á–∏—Å—Ç–∫–∞ –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤–≤–µ–¥–µ–Ω–Ω–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        cleaned_input = clean_text(user_input)
        input_tfidf = vectorizer.transform([cleaned_input])

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤—Å–µ–º–∏ –º–æ–¥–µ–ª—è–º–∏
        predictions = [model.predict(input_tfidf)[0] for model in models.values()]

        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è - —ç—Ç–æ —Å–∫–∞–ª—è—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        predictions = [pred.item() if isinstance(pred, np.ndarray) else pred for pred in predictions]

        # –í—ã–±–∏—Ä–∞–µ–º —Å–∞–º—ã–π —á–∞—Å—Ç—ã–π –∫–ª–∞—Å—Å (–≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞)
        most_common_pred = Counter(predictions).most_common(1)[0][0]
        predicted_category = label_encoder.inverse_transform([most_common_pred])[0]

        # –í—ã–≤–æ–¥ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        st.write(f"### –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π —Ä–∞–∑–¥–µ–ª –Ω–∞—É–∫–∏: **{predicted_category}**")

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        top_keywords = get_top_keywords(input_tfidf, vectorizer, top_n=5)
        st.write("### –í–æ–∑–º–æ–∂–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ (–ª–µ–º–º—ã):")
        st.write(", ".join(top_keywords) if top_keywords else "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")

        # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π
        similarities = cosine_similarity(input_tfidf, all_titles_tfidf)[0]
        top_indices = np.argsort(similarities)[::-1][:5]  # –¢–æ–ø-5 –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞

        st.write("### –¢–æ–ø-5 –ø–æ—Ö–æ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π:")
        found_similar_articles = False
        for idx in top_indices:
            similarity_score = similarities[idx]
            if similarity_score > 0:  # –¢–æ–ª—å–∫–æ —Å—Ç–∞—Ç—å–∏ —Å –Ω–µ–Ω—É–ª–µ–≤—ã–º —Å—Ö–æ–¥—Å—Ç–≤–æ–º
                found_similar_articles = True
                title = articles_data.iloc[idx]['title']
                category = label_encoder.inverse_transform([articles_data.iloc[idx]['category']])[0]
                st.write(f"- **{title}** (–†–∞–∑–¥–µ–ª: {category}, –°—Ö–æ–¥—Å—Ç–≤–æ: {similarity_score:.4f})")

        if not found_similar_articles:
            st.write("–ü–æ—Ö–æ–∂–∏—Ö —Å—Ç–∞—Ç–µ–π –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

        # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±–ª–∞–∫–∞ —Å–ª–æ–≤ –¥–ª—è –≤–≤–µ–¥–µ–Ω–Ω–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        st.write("### –û–±–ª–∞–∫–æ —Å–ª–æ–≤ –¥–ª—è –≤–∞—à–µ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞:")
        create_word_cloud(cleaned_input)

    else:
        st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏.")

st.markdown("---")
st.markdown("–°–¥–µ–ª–∞–Ω–æ —Å ‚ù§Ô∏è –∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Streamlit –∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.")